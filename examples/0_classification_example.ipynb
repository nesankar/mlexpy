{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example `mlxpy` usage on the iris dataset for multi-class classification using a RandomForest and SGDClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mlexpy (Python 3.9.5)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/NathanSankary/.pyenv/versions/3.9.5/envs/mlexpy/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# First, load the dataset, models, and mlexpy modules...\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from sklearn.datasets import load_iris\n",
    "from mlexpy import experiment, pipeline_utils, processor\n",
    "\n",
    "from typing import List, Optional, Union, Callable, Type\n",
    "\n",
    "# load a random forest and sgd classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# and numpy, pandas\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, an example of the general method flow with `mlexpy` (as described in the `README`):\n",
    "1. Load in the dataset\n",
    "2. Create your training and testing set split -- this results in an imutable named tuple structure termed an `ExperimentSetup`, this is made up of 2 `MLSetup` named tuples. Each `MLSetup` named tuple has 2 attributes, a `.obs` attribute,  and a `.labels` attribute. In essence the `.obs` attribute is your feature set (in `mlexpy` this is a pandas DataFrame, and the `.labels` is a pandas Series). An `ExperimentSetup` thus contains an `MLSetup` to use for training, and an `MLSetup` to use _purely_ for testing. This is meant to simply, and in pythonic clear language differentiate the training data (as `ExpiramentSetup.training_data`) and the test data (`ExperimentSetup.test_data`).\n",
    "    - Note: `mlexpy` defers to using a stratified train test split to retain class imbalance / class proporting in training at testing.\n",
    "3. Defing a class to do the data processing / feature engineering that inherits the `mlexpy.processor.ProcessPipelineBase` class; and a class to do the model training that inherits the `mlexpy.expirament.ClassifierExpiramentBase` class. (The explicit notebook cells will better outline this usage.)\n",
    "\n",
    "    - `mlexpy` operates in an object oriented framework. These baseclasses are built to carry a large amount of convieneint, clear, and reproducable behavior.\n",
    "\n",
    "4. Perform your feature engineering, and perform your model training.\n",
    "5. Evaluate your model.\n",
    "\n",
    "### (1) We will see how this works with all of your dev in a jupyter notebook:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mlexpy (Python 3.9.5)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/NathanSankary/.pyenv/versions/3.9.5/envs/mlexpy/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# First, set the random seed(s) for the exprament\n",
    "MODEL_SEED = 10\n",
    "PROCESS_SEED = 100\n",
    "\n",
    "model_rs = np.random.RandomState(MODEL_SEED)\n",
    "\n",
    "# First, read in the dataset as a dataframe. Because mlexpy is meant to be an exploratory/experimental tool, \n",
    "# dataframes are preferred for their readability.\n",
    "data = load_iris(as_frame=True)\n",
    "features = data[\"data\"]\n",
    "labels = data[\"target\"]\n",
    "\n",
    "# We want to look at the dataset for any faulty records...\n",
    "print(features.isna().sum())\n",
    "\n",
    "# Spoiler -- there are none in the features. Next look in the labels...\n",
    "print(labels.isna().sum())\n",
    "\n",
    "# Spoiler -- none again, so we use all data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mlexpy (Python 3.9.5)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/NathanSankary/.pyenv/versions/3.9.5/envs/mlexpy/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now, generate the ExperimentSetup object, that splits the dataset for training and testing.\n",
    "experiment_setup = pipeline_utils.get_stratified_train_test_data(train_data=features, label_data=labels, test_frac=0.35, random_state=PROCESS_SEED)\n",
    "\n",
    "# This provides us with a named tuple, with attributes of .train_data and .test_data \n",
    "# each one with attributes of .obs and .labels. For example...\n",
    "train_label_count = experiment_setup.train_data.labels.shape[0]\n",
    "test_label_count = experiment_setup.test_data.labels.shape[0]\n",
    "total_data_count = features.shape[0]\n",
    "\n",
    "print(f\"Train labels are {round((total_data_count - train_label_count) / total_data_count * 100, 2)}% of the original data ({train_label_count}).\")\n",
    "print(f\"Test labels are {round((total_data_count - test_label_count) / total_data_count * 100, 2)}% of the original data ({test_label_count}).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mlexpy (Python 3.9.5)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/NathanSankary/.pyenv/versions/3.9.5/envs/mlexpy/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Now, define the processing class. This inherits from the `ProcessPipelineBase` class. \n",
    "# For minimal functionality, this class simply needs the `.process_data()` method to be defined. Not adding \n",
    "# code for this class will result in a `NotImplementedError`.\n",
    "\n",
    "# The following shows an example of how to use this class:\n",
    "class IrisPipeline(processor.ProcessPipelineBase):\n",
    "    def __init__(self, \n",
    "        process_tag: str = \"iris_development\", \n",
    "        model_dir: Optional[Union[str, Path]] = None, \n",
    "        model_storage_function: Optional[Callable] = None, \n",
    "        model_loading_function: Optional[Callable] = None, \n",
    "        store_models: bool = True,\n",
    "        rand_int: int=10,\n",
    "        ) -> None:\n",
    "        super().__init__(process_tag, model_dir, model_storage_function, model_loading_function, store_models, rand_int)\n",
    "\n",
    "\n",
    "    # Now -- define the .process_data() method.\n",
    "    def process_data(self, df: pd.DataFrame, training: bool = True, label_series: Optional[pd.Series] = None) -> pd.DataFrame:\n",
    "        # Now, simply do all feature engineering in this method, and return the final data/feature set to perform\n",
    "        # predictions on.\n",
    "\n",
    "        # Imagine we have 1 desired feature to engineer, petal/sepal area, and then normalize the feature values.\n",
    "        # We need to pay attention in the normalizing step, because we can ONLY apply the normalize to the test\n",
    "        # set, thus we will have a fork in the process when doing the feature normalization. \n",
    "        \n",
    "        # In order to easily maintain reproducibility in data processing, any model based feature engineering (such\n",
    "        # as normalization) is done by creating a specific data structure storing the order of steps for processing each column, \n",
    "        # and the model that should be applied. This is somewhat similar to the ColumnTransformer in sklearn.\n",
    "\n",
    "        # Model based features are handled in the .fit_model_based_features() method, described below.\n",
    "         \n",
    "        # Lets begin:\n",
    "\n",
    "        # Do a copy of the passed df\n",
    "        df = df.copy()\n",
    "\n",
    "        # First, compute the petal / sepal areas (but make the columns simpler)\n",
    "        df.columns = [col.replace(\" \", \"_\").strip(\"_(cm)\") for col in df.columns]\n",
    "\n",
    "        for object in [\"petal\", \"sepal\"]:\n",
    "            df[f\"{object}_area\"] = df[f\"{object}_length\"] * df[f\"{object}_width\"]\n",
    "\n",
    "        # Now perform the training / testing dependent feature processing. This is why a `training` boolean is passed.\n",
    "        if training:\n",
    "            # Now FIT all of the model based features...\n",
    "            self.fit_model_based_features(df)\n",
    "            # ... and get the results of a transformation of all model based features.\n",
    "            model_features = self.transform_model_based_features(df)\n",
    "        else:\n",
    "            # Here we can ONLY apply the transformation\n",
    "            model_features = self.transform_model_based_features(df)\n",
    "\n",
    "        # Imagine we only want to use ONLY the scaled features for prediction, then we retrieve only the scaled columns.\n",
    "        # (This is easy because the columns are renamed with the model name in the column name)\n",
    "        prediction_df = model_features[[col for col in model_features if \"standardscaler\" in col.lower()]]\n",
    "\n",
    "        return prediction_df\n",
    "\n",
    "    def fit_model_based_features(self, df: pd.DataFrame) -> None:\n",
    "        # Here we do any processing of columns that will require a model based transformation / engineering.\n",
    "\n",
    "        # In this case, simply fit a standard (normalization) scaler to the numerical columns. \n",
    "        # This case will result in additional columns on the dataframe named as \n",
    "        # \"<original-column-name>_StandardScaler()\".\n",
    "\n",
    "        # Note: there are no returned values for this method, the result is an update in the self.column_transformations dictionary\n",
    "        for column in df.columns:\n",
    "            if df[column].dtype not in (\"float\", \"int\"):\n",
    "                continue\n",
    "            self.fit_scaler(df[column], standard_scaling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mlexpy (Python 3.9.5)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/NathanSankary/.pyenv/versions/3.9.5/envs/mlexpy/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# As an example, lets look at the outputs of the `.process_data()` method.\n",
    "iris_processor = IrisPipeline(model_dir=Path.cwd(), rand_int=PROCESS_SEED)  # set the model path to the examples directory\n",
    "\n",
    "# now run the process_data method\n",
    "processed_df = iris_processor.process_data(df=experiment_setup.train_data.obs.copy(), training=True)\n",
    "\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mlexpy (Python 3.9.5)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/NathanSankary/.pyenv/versions/3.9.5/envs/mlexpy/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Now, our \"work\" is done, lets pass our data through this process! Lets try using a randomforest model\n",
    "\n",
    "# Define the experiment\n",
    "experiment_obj = experiment.ClassifierExperiment(\n",
    "    train_setup=experiment_setup.train_data,\n",
    "    test_setup=experiment_setup.test_data,\n",
    "    cv_split_count=20,\n",
    "    model_tag=\"example_development_model\",\n",
    "    process_tag=\"example_development_process\",\n",
    "    model_dir=Path.cwd()\n",
    ")\n",
    "\n",
    "# Set the pipeline attribute to use\n",
    "experiment_obj.set_pipeline(IrisPipeline)\n",
    "\n",
    "# Now begin the experimentation, start with performing the data processing...\n",
    "processed_datasets = experiment_obj.process_data()\n",
    "\n",
    "# ... then train our model...\n",
    "trained_model = experiment_obj.train_model(\n",
    "    RandomForestClassifier(random_state=model_rs),  # This is why we have 2 different random states...\n",
    "    processed_datasets,\n",
    "    # model_algorithm.hyperparams,  # If this is passed, then cross validation search is performed, but slow.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mlexpy (Python 3.9.5)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/NathanSankary/.pyenv/versions/3.9.5/envs/mlexpy/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Now, evaluate the predictions, ClassificationExperiment provides some standard classification metrics\n",
    "# and evaluations.\n",
    "\n",
    "# Get the predictions and evaluate the performance.\n",
    "predictions = experiment_obj.predict(processed_datasets, trained_model)\n",
    "class_probabilities = experiment_obj.predict(processed_datasets, trained_model, proba=True)\n",
    "results = experiment_obj.evaluate_predictions(\n",
    "    processed_datasets.test_data.labels,\n",
    "    predictions=predictions,\n",
    "    class_probabilities=class_probabilities,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Next, how this same process might look when developing as modules. \n",
    "We now use the exact same model and dataset, however use the imported modules as our classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mlexpy (Python 3.9.5)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/NathanSankary/.pyenv/versions/3.9.5/envs/mlexpy/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# The only change is that we need to now import the classes we developed above classes.\n",
    "# Rename them for clarity of what is doing what\n",
    "from from_module_example import IrisPipeline as IrisPipeImport\n",
    "\n",
    "# First, reset our seeds...\n",
    "model_rs = np.random.RandomState(MODEL_SEED)\n",
    "\n",
    "\n",
    "# Define the experiment\n",
    "imported_experiment = experiment.ClassifierExperiment(\n",
    "    train_setup=experiment_setup.train_data,\n",
    "    test_setup=experiment_setup.test_data,\n",
    "    cv_split_count=20,\n",
    "    model_tag=\"example_development_model\",\n",
    "    process_tag=\"example_development_process\",\n",
    "    model_dir=Path.cwd()\n",
    ")\n",
    "\n",
    "# Set the pipeline to use\n",
    "imported_experiment.set_pipeline(IrisPipeImport)\n",
    "\n",
    "# Now begin the experimentation, start with performing the data processing...\n",
    "processed_datasets = imported_experiment.process_data()\n",
    "\n",
    "# ... then train the model...\n",
    "trained_model = imported_experiment.train_model(\n",
    "    RandomForestClassifier(random_state=model_rs),  # This is why we have 2 different random states...\n",
    "    processed_datasets,\n",
    "    # model_algorithm.hyperparams,  # If this is passed, then cross validation search is performed, but slow.\n",
    ")\n",
    "\n",
    "# Get the predictions and evaluate the performance.\n",
    "predictions = imported_experiment.predict(processed_datasets, trained_model)\n",
    "class_probabilities = imported_experiment.predict(processed_datasets, trained_model, proba=True)\n",
    "results = imported_experiment.evaluate_predictions(\n",
    "    processed_datasets.test_data.labels,\n",
    "    predictions=predictions,\n",
    "    class_probabilities=class_probabilities,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And you can se that we get the exact same results when using the imported modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Do the same process using this time and `SGDClassifier`\n",
    "\n",
    "This time, all we need to do is change the model that is passed when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mlexpy (Python 3.9.5)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/NathanSankary/.pyenv/versions/3.9.5/envs/mlexpy/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# The only change is that we need to now import the classes we developed above classes.\n",
    "# Rename them for clarity of what is doing what\n",
    "\n",
    "# Again, reset our seeds...\n",
    "model_rs = np.random.RandomState(MODEL_SEED)\n",
    "\n",
    "\n",
    "# Define the experiment\n",
    "imported_experiment = experiment.ClassifierExperiment(\n",
    "    train_setup=experiment_setup.train_data,\n",
    "    test_setup=experiment_setup.test_data,\n",
    "    cv_split_count=20,\n",
    "    model_tag=\"example_development_model\",\n",
    "    process_tag=\"example_development_process\",\n",
    "    model_dir=Path.cwd()\n",
    ")\n",
    "\n",
    "imported_experiment.set_pipeline(IrisPipeImport)\n",
    "# Now begin the experimentation, start with performing the data processing...\n",
    "processed_datasets = imported_experiment.process_data()\n",
    "\n",
    "# ... then train the model...\n",
    "trained_model = imported_experiment.train_model(\n",
    "    SGDClassifier(random_state=model_rs, loss=\"log\"),  # This is why we have 2 different random states...\n",
    "    processed_datasets,\n",
    "    # model_algorithm.hyperparams,  # If this is passed, then cross validation search is performed, but slow.\n",
    ")\n",
    "\n",
    "# Get the predictions and evaluate the performance.\n",
    "predictions = imported_experiment.predict(processed_datasets, trained_model)\n",
    "class_probabilities = imported_experiment.predict(processed_datasets, trained_model, proba=True)\n",
    "results = imported_experiment.evaluate_predictions(\n",
    "    processed_datasets.test_data.labels,\n",
    "    predictions=predictions,\n",
    "    class_probabilities=class_probabilities,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Now what if we wanted to use a different set of columns (ex. ALL cols not only the scaled columns)?\n",
    "We do that simply by re-defining the method to process our data. We can either, overwrite the method with the change, or compute a new process_data method for this specific case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'mlexpy (Python 3.9.5)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/NathanSankary/.pyenv/versions/3.9.5/envs/mlexpy/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# All we need to do is change the method that is called to perform our data processing below\n",
    "from from_module_example import IrisPipeline as IrisPipeImport\n",
    "\n",
    "# Again, reset our seeds...\n",
    "model_rs = np.random.RandomState(MODEL_SEED)\n",
    "\n",
    "\n",
    "# Define the experiment\n",
    "imported_experiment = experiment.ClassifierExperiment(\n",
    "    train_setup=experiment_setup.train_data,\n",
    "    test_setup=experiment_setup.test_data,\n",
    "    cv_split_count=20,\n",
    "    model_tag=\"example_development_model\",\n",
    "    process_tag=\"example_development_process\",\n",
    "    model_dir=Path.cwd()\n",
    ")\n",
    "\n",
    "imported_experiment.set_pipeline(IrisPipeImport)\n",
    "\n",
    "# Now begin the experimentation, however, we here provide a string corresponding to a method name\n",
    "# to use to do the data processing.\n",
    "#  Not providing any process_method_str value will default to using \"process_data\"\n",
    "processed_datasets = imported_experiment.process_data(process_method_str=\"process_data_keep_all_columns\")\n",
    "\n",
    "# ... then train the model...\n",
    "trained_model = imported_experiment.train_model(\n",
    "    SGDClassifier(random_state=model_rs, loss=\"log\"),  # This is why we have 2 different random states...\n",
    "    processed_datasets,\n",
    "    # model_algorithm.hyperparams,  # If this is passed, then cross validation search is performed, but slow.\n",
    ")\n",
    "\n",
    "# Get the predictions and evaluate the performance.\n",
    "predictions = imported_experiment.predict(processed_datasets, trained_model)\n",
    "class_probabilities = imported_experiment.predict(processed_datasets, trained_model, proba=True)\n",
    "results = imported_experiment.evaluate_predictions(\n",
    "    processed_datasets.test_data.labels,\n",
    "    predictions=predictions,\n",
    "    class_probabilities=class_probabilities,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, using all of the data results in better scored for the `SGDClassifier`. (We confrim that the data was processed differently looking at the log of the train and test data shape (`The train data are of size (97, 12), the test data are (53, 12).`))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7d53b7080da918b862bf2b02e1f266275bbc172f0f94ff6a82790353fa275a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
